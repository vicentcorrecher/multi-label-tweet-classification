{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8825836,"sourceType":"datasetVersion","datasetId":5215525}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Tarea PLN**","metadata":{}},{"cell_type":"markdown","source":"En este notebook se presenta la parte de entrenamiento del modelo seleccionado: Ensemble Learning y la predicción final sobre el dataset de *sem_eval_test_blank_es*","metadata":{}},{"cell_type":"markdown","source":"### Importación de datos","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n# Cargar el conjunto de datos de entrenamiento\ndf = pd.read_csv('/kaggle/input/trainn/sem_eval_train_es.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:39:03.545564Z","iopub.execute_input":"2024-06-30T16:39:03.545919Z","iopub.status.idle":"2024-06-30T16:39:03.569909Z","shell.execute_reply.started":"2024-06-30T16:39:03.545890Z","shell.execute_reply":"2024-06-30T16:39:03.569145Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Entrenamiento Ensemble Learning","metadata":{}},{"cell_type":"code","source":"import joblib\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\nimport os\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Descargar stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('spanish'))\n\n# Función para limpiar texto\ndef limpiar_texto(text):\n    text = text.lower()\n    text = re.sub(r'(@\\w+|#\\w+|http\\S+)', '', text)\n    text = re.sub(r'[^a-záéíóúñü\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Función para eliminar stopwords\ndef eliminar_stopwords(text):\n    tokens = text.split()\n    tokens = [word for word in tokens if word not in stop_words]\n    return ' '.join(tokens)\n\n# Cargar el conjunto de datos de entrenamiento\ndf = pd.read_csv('/kaggle/input/trainn/sem_eval_train_es (1).csv')\ndf['Tweet'] = df['Tweet'].apply(limpiar_texto).apply(eliminar_stopwords)\n\n# Separar las características y las etiquetas\nX = df['Tweet']\ny = df.iloc[:, 2:]\n\n# Dividir los datos\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Cargar los tokenizadores y modelos\nbert_tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n\n# Preparar los datos para los dos modelos\nclass BERTDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts.iloc[idx]\n        labels = self.labels.iloc[idx].values.astype(float)\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(labels, dtype=torch.float)\n        }\n\n# Crear datasets para BERT\ntrain_dataset_bert = BERTDataset(X_train, y_train, bert_tokenizer, max_len=128)\ntest_dataset_bert = BERTDataset(X_test, y_test, bert_tokenizer, max_len=128)\n\n# Crear datasets para XLM-Roberta\ntrain_dataset_xlm = BERTDataset(X_train, y_train, xlm_tokenizer, max_len=128)\ntest_dataset_xlm = BERTDataset(X_test, y_test, xlm_tokenizer, max_len=128)\n\n# Definir los modelos para clasificación multietiqueta\nmodel_bert = BertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', num_labels=y_train.shape[1])\nmodel_xlm = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=y_train.shape[1])\n\n# Configurar el entrenamiento\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=8,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    learning_rate=3e-5,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",  # Guardar solo al final de cada epoch\n    save_steps=5000,  # Guardar con menos frecuencia\n    save_total_limit=1,  # Mantener solo el último checkpoint\n)\n\n# Entrenar el modelo BERT\ntrainer_bert = Trainer(\n    model=model_bert,\n    args=training_args,\n    train_dataset=train_dataset_bert,\n    eval_dataset=test_dataset_bert,\n)\ntrainer_bert.train()\n\n# Evaluar el modelo BERT\npredictions_bert = trainer_bert.predict(test_dataset_bert)\nprobabilities_bert = torch.sigmoid(torch.tensor(predictions_bert.predictions)).numpy()\n\n# Guardar el modelo BERT\nmodel_bert.save_pretrained('/kaggle/working/bert_model')\nbert_tokenizer.save_pretrained('/kaggle/working/bert_tokenizer')\n\n# Liberar memoria de BERT\ndel model_bert\ntorch.cuda.empty_cache()\n\n# Configurar el entrenamiento para XLM-Roberta\ntraining_args.save_strategy = \"no\"  # No guardar checkpoints intermedios\n\n# Entrenar el modelo XLM-Roberta\ntrainer_xlm = Trainer(\n    model=model_xlm,\n    args=training_args,\n    train_dataset=train_dataset_xlm,\n    eval_dataset=test_dataset_xlm,\n)\ntrainer_xlm.train()\n\n# Evaluar el modelo XLM-Roberta\npredictions_xlm = trainer_xlm.predict(test_dataset_xlm)\nprobabilities_xlm = torch.sigmoid(torch.tensor(predictions_xlm.predictions)).numpy()\n\n# Guardar el modelo XLM-Roberta\nmodel_xlm.save_pretrained('/kaggle/working/xlm_model')\nxlm_tokenizer.save_pretrained('/kaggle/working/xlm_tokenizer')\n\n# Promediar las probabilidades de los dos modelos para el ensemble\nprobabilities_ensemble = (probabilities_bert + probabilities_xlm) / 2\n\n# Optimizar los umbrales para cada etiqueta\noptimal_thresholds = []\nfor i in range(y_train.shape[1]):\n    best_threshold = 0.5\n    best_f1 = 0\n    for threshold in np.arange(0.1, 0.9, 0.01):\n        y_pred_bin = (probabilities_ensemble[:, i] >= threshold).astype(int)\n        f1 = f1_score(y_test.iloc[:, i], y_pred_bin)\n        if f1 > best_f1:\n            best_f1 = f1\n            best_threshold = threshold\n    optimal_thresholds.append(best_threshold)\n\n# Guardar los umbrales óptimos\njoblib.dump(optimal_thresholds, '/kaggle/working/optimal_thresholds.pkl')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:50:54.145037Z","iopub.execute_input":"2024-06-30T16:50:54.145601Z","iopub.status.idle":"2024-06-30T17:11:01.921736Z","shell.execute_reply.started":"2024-06-30T16:50:54.145568Z","shell.execute_reply":"2024-06-30T17:11:01.920732Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1424' max='1424' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1424/1424 07:46, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.376100</td>\n      <td>0.387972</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.343200</td>\n      <td>0.341893</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.314500</td>\n      <td>0.319048</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.239300</td>\n      <td>0.318191</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.204700</td>\n      <td>0.315419</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.173800</td>\n      <td>0.323340</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.126900</td>\n      <td>0.331108</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.134200</td>\n      <td>0.331562</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1424' max='1424' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1424/1424 11:59, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.385700</td>\n      <td>0.391638</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.389000</td>\n      <td>0.383708</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.378400</td>\n      <td>0.364458</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.343100</td>\n      <td>0.354653</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.321300</td>\n      <td>0.332313</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.303500</td>\n      <td>0.326647</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.276300</td>\n      <td>0.330140</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.309400</td>\n      <td>0.327486</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/optimal_thresholds.pkl']"},"metadata":{}}]},{"cell_type":"markdown","source":"### Predecir sobre el dataset final","metadata":{}},{"cell_type":"code","source":"import os\nimport joblib\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom torch.utils.data import Dataset\n\n# Descargar stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('spanish'))\n\n# Función para limpiar texto\ndef limpiar_texto(text):\n    text = text.lower()\n    text = re.sub(r'(@\\w+|#\\w+|http\\S+)', '', text)\n    text = re.sub(r'[^a-záéíóúñü\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Función para eliminar stopwords\ndef eliminar_stopwords(text):\n    tokens = text.split()\n    tokens = [word for word in tokens if word not in stop_words]\n    return ' '.join(tokens)\n\n# Definir la clase de dataset\nclass BERTDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts.iloc[idx]\n        labels = self.labels.iloc[idx].values.astype(float)\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(labels, dtype=torch.float)\n        }\n\n# Cargar el conjunto de datos de test\ntest_df = pd.read_csv('/kaggle/input/trainn/sem_eval_test_blank_es.csv')\ntest_df['Tweet'] = test_df['Tweet'].apply(limpiar_texto).apply(eliminar_stopwords)\nX_test = test_df['Tweet']\ntweet_ids = test_df['ID']\n\n# Crear dataset para predicción\ntest_dataset_bert = BERTDataset(X_test, pd.DataFrame(np.zeros((X_test.shape[0], len(optimal_thresholds)))), bert_tokenizer, max_len=128)\ntest_dataset_xlm = BERTDataset(X_test, pd.DataFrame(np.zeros((X_test.shape[0], len(optimal_thresholds)))), xlm_tokenizer, max_len=128)\n\n# Cargar los modelos y los tokenizadores\nbert_tokenizer = BertTokenizer.from_pretrained('/kaggle/working/bert_tokenizer')\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained('/kaggle/working/xlm_tokenizer')\nmodel_bert = BertForSequenceClassification.from_pretrained('/kaggle/working/bert_model', num_labels=len(optimal_thresholds))\nmodel_xlm = XLMRobertaForSequenceClassification.from_pretrained('/kaggle/working/xlm_model', num_labels=len(optimal_thresholds))\n\n# Cargar los umbrales óptimos\noptimal_thresholds = joblib.load('/kaggle/working/optimal_thresholds.pkl')\n\n# Configurar los entrenadores para predicción\ntrainer_bert = Trainer(model=model_bert)\ntrainer_xlm = Trainer(model=model_xlm)\n\n# Obtener las predicciones\npredictions_bert = trainer_bert.predict(test_dataset_bert)\nprobabilities_bert = torch.sigmoid(torch.tensor(predictions_bert.predictions)).numpy()\n\npredictions_xlm = trainer_xlm.predict(test_dataset_xlm)\nprobabilities_xlm = torch.sigmoid(torch.tensor(predictions_xlm.predictions)).numpy()\n\n# Promediar las probabilidades\nprobabilities_ensemble = (probabilities_bert + probabilities_xlm) / 2\n\n# Convertir probabilidades a etiquetas binarias utilizando los umbrales óptimos\ny_pred_bin = np.zeros(probabilities_ensemble.shape)\nfor i in range(probabilities_ensemble.shape[1]):\n    y_pred_bin[:, i] = (probabilities_ensemble[:, i] >= optimal_thresholds[i]).astype(int)\n\n# Crear el DataFrame con las predicciones\npred_df = pd.DataFrame(y_pred_bin, columns=['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'])\npred_df.insert(0, 'ID', tweet_ids)\npred_df = pred_df.astype({col: 'bool' for col in pred_df.columns if col != 'ID'})\n\n# Guardar el DataFrame en un archivo CSV\npred_df.to_csv('/kaggle/working/soluciones_Vicent_Munoz_Correcher.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T17:17:40.464560Z","iopub.execute_input":"2024-06-30T17:17:40.464896Z","iopub.status.idle":"2024-06-30T17:17:53.173868Z","shell.execute_reply.started":"2024-06-30T17:17:40.464872Z","shell.execute_reply":"2024-06-30T17:17:53.173097Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]}]}